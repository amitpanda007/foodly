# Foodly - AI-Powered Recipe Narration App

A full-stack web application that transforms recipe URLs and YouTube videos into step-by-step audio and text narrations. Built with React, TypeScript, FastAPI, and Ollama for local AI processing.

## Features

- ğŸ”— **URL & YouTube Support**: Paste any recipe URL or YouTube cooking video
- ğŸ¯ **Step-by-Step Guide**: Get organized, easy-to-follow recipe steps
- ğŸ¤ **Voice Commands**: Navigate through steps using voice ("next", "back", "repeat")
- ğŸ”Š **Audio Narration**: Listen to each step with text-to-speech
- ğŸ“± **Mobile Friendly**: Responsive design optimized for cooking in the kitchen
- ğŸŒ™ **Dark/Light Mode**: Toggle between themes for comfortable viewing
- ğŸ“± **Screen Wake Lock**: Keep your screen on while cooking
- ğŸ” **Recipe Search**: Search through your saved recipes
- ğŸ¤– **Local AI**: Uses Ollama for privacy-focused local processing
- â˜ï¸ **Cloud AI Options**: Optional support for OpenAI and Google Gemini

## Tech Stack

### Backend
- **FastAPI** - High-performance async Python framework
- **PostgreSQL** - Robust relational database
- **SQLAlchemy** - Async ORM for database operations
- **Ollama** - Local LLM inference
- **BeautifulSoup** - Web scraping for recipe extraction
- **youtube-transcript-api** - YouTube video transcript extraction

### Frontend
- **React 18** - UI framework with hooks
- **TypeScript** - Type-safe JavaScript
- **Vite** - Fast build tool
- **Tailwind CSS** - Utility-first CSS framework
- **Web Speech API** - Voice recognition and synthesis
- **Screen Wake Lock API** - Keep screen active

## Project Structure

```
foodly/
â”œâ”€â”€ docker-compose.yml          # Full production deployment
â”œâ”€â”€ docker-compose.dev.yml      # Development (DB + Ollama only)
â”œâ”€â”€ env.example                 # Docker Compose env template
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ env.example             # Backend env template
â”‚   â””â”€â”€ app/
â”‚       â”œâ”€â”€ main.py
â”‚       â”œâ”€â”€ config.py
â”‚       â”œâ”€â”€ database.py
â”‚       â”œâ”€â”€ models/
â”‚       â”œâ”€â”€ routers/
â”‚       â”œâ”€â”€ schemas/
â”‚       â””â”€â”€ services/
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ env.example             # Frontend env template
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ App.tsx
â”‚       â”œâ”€â”€ components/
â”‚       â”œâ”€â”€ hooks/
â”‚       â”œâ”€â”€ pages/
â”‚       â””â”€â”€ services/
â”‚
â””â”€â”€ scripts/
    â”œâ”€â”€ start-local.ps1         # Windows local dev
    â”œâ”€â”€ start-local.sh          # Linux/Mac local dev
    â”œâ”€â”€ setup-ollama.ps1        # Windows Ollama setup
    â””â”€â”€ setup-ollama.sh         # Linux/Mac Ollama setup
```

## Quick Start

### Prerequisites

- Docker & Docker Compose (recommended)
- OR Node.js 18+ and Python 3.11+
- PostgreSQL 16 (for local development)
- Ollama (for local AI)

### Docker Deployment (Recommended)

1. Clone and navigate to the project:
   ```bash
   cd foodly
   ```

2. Copy environment file:
   ```bash
   cp env.example .env
   ```

3. Start all services:
   ```bash
   docker-compose up -d
   ```

4. Access the application:
   - Frontend: http://localhost:3000
   - Backend API: http://localhost:8000
   - API Docs: http://localhost:8000/docs

### Local Development

#### Option 1: Using Helper Script (Recommended)

**Windows (PowerShell):**
```powershell
# Start database and Ollama first
docker-compose -f docker-compose.dev.yml up -d

# Setup Ollama model
.\scripts\setup-ollama.ps1

# Start both frontend and backend
.\scripts\start-local.ps1
```

**Linux/Mac:**
```bash
# Start database and Ollama first
docker-compose -f docker-compose.dev.yml up -d

# Setup Ollama model
./scripts/setup-ollama.sh

# Start both frontend and backend
./scripts/start-local.sh
```

#### Option 2: Manual Setup

**Backend:**
```bash
cd backend

# Create and activate virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# or: .\venv\Scripts\activate  # Windows

# Install dependencies
pip install -r requirements.txt

# Copy env file
cp env.example .env

# Run the server
uvicorn app.main:app --reload --port 8000
```

**Frontend:**
```bash
cd frontend

# Install dependencies
npm install

# Copy env file
cp env.example .env

# Run dev server
npm run dev
```

## Configuration

### Environment Files

| File | Purpose |
|------|---------|
| `env.example` | Docker Compose variables (PostgreSQL, build args) |
| `backend/env.example` | Backend API configuration |
| `frontend/env.example` | Frontend Vite configuration |

### Backend Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `DATABASE_URL` | PostgreSQL connection string | `postgresql+asyncpg://...` |
| `OLLAMA_BASE_URL` | Ollama API endpoint | `http://localhost:11434` |
| `OLLAMA_MODEL` | Ollama model to use | `llama3.2:3b` |
| `LLM_PROVIDER` | AI provider: `ollama`, `openai`, `gemini` | `ollama` |
| `OPENAI_API_KEY` | OpenAI API key (if using OpenAI) | - |
| `GEMINI_API_KEY` | Google Gemini API key (if using Gemini) | - |
| `CORS_ORIGINS` | Allowed CORS origins | `http://localhost:5173,...` |

### Frontend Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `VITE_API_URL` | Backend API URL | `http://localhost:8000` |

### Switching LLM Providers

Edit `backend/.env`:

```bash
# For local Ollama (default)
LLM_PROVIDER=ollama
OLLAMA_MODEL=llama3.2:3b

# For OpenAI
LLM_PROVIDER=openai
OPENAI_API_KEY=sk-your-key-here

# For Google Gemini
LLM_PROVIDER=gemini
GEMINI_API_KEY=your-gemini-key-here
```

## Usage

1. **Add a Recipe**: Paste a recipe URL or YouTube video link
2. **Wait for Processing**: AI extracts and structures the recipe
3. **Follow Steps**: Use buttons or voice commands to navigate
4. **Voice Commands**:
   - "Next" / "Next step" - Go to next step
   - "Back" / "Previous" - Go to previous step
   - "Repeat" - Read current step again
5. **Mark Complete**: Check off steps as you complete them
6. **Screen Wake Lock**: Toggle to keep screen on while cooking

## API Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/api/recipes/process` | Process a new recipe URL |
| `GET` | `/api/recipes` | Get all saved recipes |
| `GET` | `/api/recipes/{id}` | Get specific recipe |
| `DELETE` | `/api/recipes/{id}` | Delete a recipe |
| `GET` | `/api/recipes/search?q=` | Search recipes |
| `GET` | `/api/health` | Health check |

## Contributing

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Open a Pull Request

## License

MIT License - see LICENSE file for details.
